{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1sS-Ra-rzPRC4QvPVlYQ-dwAJAjehHRTE",
      "authorship_tag": "ABX9TyMd6ZzwI5MaZ40WEfDKO8nX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PSchloss12/intro_ai_project/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sources\n",
        "- https://pytorch.org/tutorials/index.html\n",
        "- https://towardsdatascience.com/language-modeling-with-lstms-in-pytorch-381a26badcbf\n",
        "- https://writesonic.com/blog/how-to-train-chatgpt-own-data/\n",
        "- https://arxiv.org/abs/1708.02182\n",
        "\n"
      ],
      "metadata": {
        "id": "L2oRx_awBllM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# x_np = torch.from_numpy(np_array)\n",
        "# x_data = torch.tensor(data)\n",
        "\n",
        "# # We move our tensor to the GPU if available\n",
        "# if torch.cuda.is_available():\n",
        "#   tensor = tensor.to('cuda')\n",
        "\n",
        "# # print('First row: ',tensor[0])\n",
        "# # print('First column: ', tensor[:, 0])\n",
        "# # print('Last column:', tensor[..., -1])\n",
        "\n",
        "\n",
        "# # to save\n",
        "# torch.save(model.state_dict(), PATH)\n",
        "\n",
        "# # to load\n",
        "# model = TheModelClass(*args, **kwargs)\n",
        "# model.load_state_dict(torch.load(PATH))\n",
        "# model.eval()"
      ],
      "metadata": {
        "id": "jea-n8hm2OWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNILoJ232Jp4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "\n",
        "import numpy as np\n",
        "import tqdm\n",
        "\n",
        "import os\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Best Available Device\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available() # GPU\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywhi0BN43rdM",
        "outputId": "3504883f-3746-4b28-a86e-aff0430f0d2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class NeuralNetwork(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.flatten = nn.Flatten()\n",
        "#         self.linear_relu_stack = nn.Sequential(\n",
        "#             nn.Linear(28*28, 512),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(512, 512),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(512, 10),\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.flatten(x)\n",
        "#         logits = self.linear_relu_stack(x)\n",
        "#         return logits"
      ],
      "metadata": {
        "id": "400XOega32pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Create NN and move to device\n",
        "# model = NeuralNetwork().to(device)\n",
        "# print(model)\n",
        "# # get layer weights and biases\n",
        "# print(f\"Model structure: {model}\\n\\n\")\n",
        "# for name, param in model.named_parameters():\n",
        "#     print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
      ],
      "metadata": {
        "id": "FnN4ZE684Hwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "import datasets\n",
        "\n",
        "# feed in data\n",
        "# X = torch.__(device=device)\n",
        "# logits = model(X)\n",
        "# pred_probab = nn.Softmax(dim=1)(logits)\n",
        "# y_pred = pred_probab.argmax(1)\n",
        "# print(f\"Predicted class: {y_pred}\")\n",
        "\n",
        "def get_data(dataset, vocab, batch_size):\n",
        "    '''\n",
        "    Implementing the Dataloader\n",
        "    given a dataset gives a way to iterate over batches of it (In a batch, all examples are processed in parallel)\n",
        "    '''\n",
        "    data = []\n",
        "    for example in dataset:\n",
        "        if example['tokens']:\n",
        "            # appends each sequence of tokenized text with an <eos> token to mark its end\n",
        "            tokens = example['tokens'].append('<eos>')\n",
        "            # encodes each token to a numerical value equal to its index in the vocabulary; rare words match to unknown token\n",
        "            tokens = [vocab[token] for token in example['tokens']]\n",
        "            data.extend(tokens)\n",
        "    # combines all the numerical sequences into a list (1D Tensor)\n",
        "    data = torch.LongTensor(data)\n",
        "    num_batches = data.shape[0] // batch_size\n",
        "    # reshapes it into a 2D tensor of dimensions [batch_size, num_batches]\n",
        "    data = data[:num_batches * batch_size]\n",
        "    data = data.view(batch_size, num_batches)\n",
        "    return data\n",
        "\n",
        "#  load data\n",
        "files = []\n",
        "for file in os.listdir(\"/content/drive/MyDrive/AI/Project/train\"):\n",
        "  files.append(\"/content/drive/MyDrive/AI/Project/train/\"+file)\n",
        "# dataset = datasets.load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
        "\n",
        "dataset = datasets.load_dataset('text', data_files={'train': files,'test': \"/content/drive/MyDrive/AI/Project/test/red_fairybook_parsed.txt\"})\n",
        "# tokenize data, basically breaks into words and punctuation here\n",
        "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
        "tokenize_data = lambda example, tokenizer: {'tokens': tokenizer(example['text'])}\n",
        "tokenized_dataset = dataset.map(tokenize_data, remove_columns=['text'],\n",
        "fn_kwargs={'tokenizer': tokenizer})\n",
        "\n",
        "# create vocab of any word that occurs at least 3 times\n",
        "# length will be the number of neurons in the output classification layer\n",
        "vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_dataset['train']['tokens'],min_freq=3)\n",
        "# manually add an <unk> token and set is as the default index so that whenever we request from the vocabulary the index of a word that it doesn’t have we get <unk>\n",
        "vocab.insert_token('<unk>', 0)\n",
        "# add <eos> token; We will later insert it at the end of each sequence so model will learn to do so as well\n",
        "vocab.insert_token('<eos>', 1)\n",
        "vocab.set_default_index(vocab['<unk>'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEkJgdfc4a3K",
        "outputId": "10ad1a6a-9b2d-4d23-b74f-989d28c3e67e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "train_data = get_data(tokenized_dataset['train'], vocab, batch_size)\n",
        "# valid_data = get_data(tokenized_dataset['validation'], vocab, batch_size)\n",
        "test_data = get_data(tokenized_dataset['test'], vocab, batch_size)\n"
      ],
      "metadata": {
        "id": "fIMUNTw2Drsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate,\n",
        "                tie_weights):\n",
        "\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers,\n",
        "                    dropout=dropout_rate, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "        # The purpose of this is to make the embedding layer share weights with the output layer. This helps reduce the number of parameters\n",
        "        if tie_weights:\n",
        "            assert embedding_dim == hidden_dim, 'cannot tie, check dims'\n",
        "            self.embedding.weight = self.fc.weight\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, src, hidden):\n",
        "        embedding = self.dropout(self.embedding(src))\n",
        "        output, hidden = self.lstm(embedding, hidden)\n",
        "        output = self.dropout(output)\n",
        "        prediction = self.fc(output)\n",
        "        return prediction, hidden\n",
        "\n",
        "    def init_weights(self):\n",
        "      '''\n",
        "      initialize the embedding weights uniformly in the range [-0.1, 0.1]\n",
        "      and all other layers uniformly in the range [-1/sqrt(H), 1/sqrt(H)]\n",
        "      '''\n",
        "      init_range_emb = 0.1\n",
        "      init_range_other = 1/math.sqrt(self.hidden_dim)\n",
        "      self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)\n",
        "      self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
        "      self.fc.bias.data.zero_()\n",
        "      for i in range(self.num_layers):\n",
        "          self.lstm.all_weights[i][0] = torch.FloatTensor(self.embedding_dim,\n",
        "                  self.hidden_dim).uniform_(-init_range_other, init_range_other)\n",
        "          self.lstm.all_weights[i][1] = torch.FloatTensor(self.hidden_dim,\n",
        "                  self.hidden_dim).uniform_(-init_range_other, init_range_other)\n",
        "\n",
        "    def init_hidden(self, batch_size, device):\n",
        "        '''\n",
        "        set the LSTM’s hidden and cell state to zero\n",
        "        '''\n",
        "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
        "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
        "        return hidden, cell\n",
        "\n",
        "    def detach_hidden(self, hidden):\n",
        "        '''\n",
        "        need this function while training to explicitly tell PyTorch that hidden states due to different sequences are independent\n",
        "        '''\n",
        "        hidden, cell = hidden\n",
        "        hidden = hidden.detach()\n",
        "        cell = cell.detach()\n",
        "        return hidden, cell"
      ],
      "metadata": {
        "id": "Bd4Zfvme_QFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    model.eval()\n",
        "    num_batches = data.shape[-1]\n",
        "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
        "    num_batches = data.shape[-1]\n",
        "\n",
        "    hidden = model.init_hidden(batch_size, device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx in range(0, num_batches - 1, seq_len):\n",
        "            hidden = model.detach_hidden(hidden)\n",
        "            src, target = get_batch(data, seq_len, num_batches, idx)\n",
        "            src, target = src.to(device), target.to(device)\n",
        "            batch_size= src.shape[0]\n",
        "\n",
        "            prediction, hidden = model(src, hidden)\n",
        "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
        "            target = target.reshape(-1)\n",
        "\n",
        "            loss = criterion(prediction, target)\n",
        "            epoch_loss += loss.item() * seq_len\n",
        "    return epoch_loss / num_batches\n",
        "\n",
        "def get_batch(data, seq_len, num_batches, idx):\n",
        "    '''\n",
        "    given the index of the first batch of tokens in the batch returns the corresponding batch of sequences\n",
        "    '''\n",
        "    src = data[:, idx:idx+seq_len]\n",
        "    target = data[:, idx+1:idx+seq_len+1]\n",
        "    return src, target\n",
        "\n",
        "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
        "    epoch_loss = 0\n",
        "    model.train()\n",
        "    # drop all batches that are not a multiple of seq_len\n",
        "    num_batches = data.shape[-1]\n",
        "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
        "    num_batches = data.shape[-1]\n",
        "\n",
        "    hidden = model.init_hidden(batch_size, device)\n",
        "\n",
        "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):  # The last batch can't be a src\n",
        "        optimizer.zero_grad()\n",
        "        hidden = model.detach_hidden(hidden)\n",
        "\n",
        "        src, target = get_batch(data, seq_len, num_batches, idx)\n",
        "        src, target = src.to(device), target.to(device)\n",
        "        batch_size = src.shape[0]\n",
        "        prediction, hidden = model(src, hidden)\n",
        "\n",
        "        prediction = prediction.reshape(batch_size * seq_len, -1)\n",
        "        target = target.reshape(-1)\n",
        "        loss = criterion(prediction, target)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item() * seq_len\n",
        "    return epoch_loss / num_batches\n",
        "\n",
        "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "    model.eval()\n",
        "    tokens = tokenizer(prompt)\n",
        "    indices = [vocab[t] for t in tokens]\n",
        "    batch_size = 1\n",
        "    hidden = model.init_hidden(batch_size, device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(max_seq_len):\n",
        "            src = torch.LongTensor([indices]).to(device)\n",
        "            prediction, hidden = model(src, hidden)\n",
        "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)\n",
        "            prediction = torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "            while prediction == vocab['<unk>']:\n",
        "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "            if prediction == vocab['<eos>']:\n",
        "                break\n",
        "\n",
        "            indices.append(prediction)\n",
        "\n",
        "    itos = vocab.get_itos()\n",
        "    tokens = [itos[i] for i in indices]\n",
        "    return tokens\n"
      ],
      "metadata": {
        "id": "LrK9vwBEAQ0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning & Model Initialization"
      ],
      "metadata": {
        "id": "vwRsq5p-OK6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab)\n",
        "# set the embedding and hidden dimensions as the same value because we will use weight tying\n",
        "embedding_dim = 1024             # 400 in the paper\n",
        "hidden_dim = 1024                # 1150 in the paper\n",
        "num_layers = 2                   # 3 in the paper\n",
        "dropout_rate = 0.65\n",
        "tie_weights = True\n",
        "lr = 1e-3                        # They used 30 and a different optimizer\n",
        "\n",
        "# initialize the model, optimizer and loss criterion\n",
        "model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, tie_weights).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'The model has {num_params:,} trainable parameters')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cb491hFm_0ac",
        "outputId": "1c3559f6-2ecd-4331-d9a6-554458691415"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 24,170,525 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training & Evaluation"
      ],
      "metadata": {
        "id": "FFHsp3s9R2vn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 50\n",
        "seq_len = 50\n",
        "clip = 0.25\n",
        "saved = False\n",
        "\n",
        "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
        "\n",
        "if saved:\n",
        "    model.load_state_dict(torch.load('best-val-lstm_lm.pt',  map_location=device))\n",
        "    test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
        "    print(f'Test Perplexity: {math.exp(test_loss):.3f}')\n",
        "else:\n",
        "    best_valid_loss = float('inf')\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        train_loss = train(model, train_data, optimizer, criterion, batch_size, seq_len, clip, device)\n",
        "        valid_loss = evaluate(model, valid_data, criterion, batch_size, seq_len, device)\n",
        "\n",
        "        lr_scheduler.step(valid_loss)\n",
        "\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save(model.state_dict(), 'best-val-lstm_lm.pt')\n",
        "\n",
        "        print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
        "        print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "Llt2sZ_BR3Kk",
        "outputId": "18d06b3a-c2b8-4949-9ea2-957906237e21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-c34aeaa059fc>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         train_loss = train(model, train_data, optimizer, criterion, \n\u001b[0m\u001b[1;32m     17\u001b[0m                     batch_size, seq_len, clip, device)\n\u001b[1;32m     18\u001b[0m         valid_loss = evaluate(model, valid_data, criterion, batch_size, \n",
            "\u001b[0;32m<ipython-input-21-deaecf43e06e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, optimizer, criterion, batch_size, seq_len, clip, device)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batches\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# The last batch can't be a src\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt = 'Think about'\n",
        "# max_seq_len = 30\n",
        "# seed = 0\n",
        "\n",
        "# temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
        "# for temperature in temperatures:\n",
        "#     generation = generate(prompt, max_seq_len, temperature, model, tokenizer,\n",
        "#                           vocab, device, seed)\n",
        "#     print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
      ],
      "metadata": {
        "id": "vpKJvinpBAFi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}